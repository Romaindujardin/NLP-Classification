{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "031ada14",
      "metadata": {},
      "source": [
        "# Importation des packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd2e3037",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "# Import des modules Keras et sklearn\n",
        "from tensorflow.keras.layers import TextVectorization, LSTM, Dropout, Bidirectional, Dense, Embedding\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.metrics import Precision, Recall\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9fb031f9",
      "metadata": {},
      "source": [
        "# 1. Importation des données"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00afbf59",
      "metadata": {},
      "source": [
        "Ajoutez un raccourci de ce dossier à votre google drive :"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c1ff101",
      "metadata": {},
      "source": [
        "https://drive.google.com/drive/folders/1mx-CAzT10YKrmxHfYDP_1Oef7PVGUr7s?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9116d213",
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7cfb528",
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv('/Users/romain/Downloads/Classification/data_classification_commentaires_toxiques/train.csv')\n",
        "\n",
        "# Affichage rapide du dataframe et d'un exemple de commentaire toxique\n",
        "print(df)\n",
        "print(df.loc[df.identity_hate == 1].iloc[0].comment_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9cd48d33",
      "metadata": {},
      "source": [
        "# 2. PRÉTRAITEMENT : VECTORISATION DU TEXTE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3969f0fb",
      "metadata": {},
      "outputs": [],
      "source": [
        "x = df.comment_text\n",
        "y = df[df.columns[2:]].values\n",
        "\n",
        "MAX_FEATURES = 200000\n",
        "vectorizer = TextVectorization(max_tokens=MAX_FEATURES,\n",
        "                               output_sequence_length=2000,\n",
        "                               output_mode='int')\n",
        "# Adapter le vectoriseur sur les données textuelles\n",
        "vectorizer.adapt(x.values)\n",
        "print(\"Taille du vocabulaire :\", len(vectorizer.get_vocabulary()))\n",
        "print(\"Exemple de vectorisation :\", vectorizer('Hello world, life is great'))\n",
        "\n",
        "# Conversion du texte en séquences vectorisées\n",
        "vectorized_text = vectorizer(x.values)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a449287",
      "metadata": {},
      "source": [
        "# 3. CRÉATION DU DATASET TENSORFLOW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82b282df",
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices((vectorized_text, y))\n",
        "dataset = dataset.cache()\n",
        "dataset = dataset.shuffle(160000)\n",
        "dataset = dataset.batch(16)\n",
        "dataset = dataset.prefetch(8)\n",
        "\n",
        "# Affichage d'un batch pour vérification\n",
        "batch_x, batch_y = dataset.as_numpy_iterator().next()\n",
        "print(\"Shape d'un batch (x):\", batch_x.shape)\n",
        "print(\"Shape d'un batch (y):\", batch_y.shape)\n",
        "\n",
        "# Division du dataset en ensembles d'entraînement, validation et test\n",
        "total_batches = len(dataset)\n",
        "train = dataset.take(int(total_batches * 0.7))\n",
        "val = dataset.skip(int(total_batches * 0.7)).take(int(total_batches * 0.2))\n",
        "test = dataset.skip(int(total_batches * 0.9)).take(int(total_batches * 0.1))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cfdace94",
      "metadata": {},
      "source": [
        "# 4. CONSTRUCTION ET ENTRAÎNEMENT DU MODÈLE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2995bdb",
      "metadata": {},
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Embedding(MAX_FEATURES + 1, 32))\n",
        "model.add(Bidirectional(LSTM(32, activation='tanh')))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(6, activation='sigmoid'))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss='BinaryCrossentropy', optimizer='Adam', metrics=['accuracy'])\n",
        "history = model.fit(train, epochs=1, validation_data=val)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ba0f810",
      "metadata": {},
      "source": [
        "# 5. PRÉDICTIONS ET ÉVALUATIONS INITIALES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8729ab9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prédiction sur un exemple de texte\n",
        "input_text = vectorizer('You freaking suck! I am going to kill you')\n",
        "print(\"Prédiction (exemple 1) :\", model.predict(np.array([input_text])))\n",
        "print(\"Prédiction (exemple 2) :\", model.predict(np.expand_dims(input_text, 0)))\n",
        "\n",
        "# Evaluation sur un batch du test set\n",
        "batch_x, batch_y = test.as_numpy_iterator().next()\n",
        "print(\"Prédictions sur batch test :\", (model.predict(batch_x) > 0.5).astype(int))\n",
        "print(\"Labels réels :\", batch_y)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7eb62fb7",
      "metadata": {},
      "source": [
        "# 6. CALCUL DES MÉTRIQUES AVEC KERAS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0e850bc",
      "metadata": {},
      "outputs": [],
      "source": [
        "pre = Precision()\n",
        "re = Recall()\n",
        "for batch in test.as_numpy_iterator():\n",
        "    batch_x, batch_y = batch\n",
        "    yhat = model.predict(batch_x)\n",
        "    # Aplatir les tableaux pour le calcul\n",
        "    pre.update_state(batch_y.flatten(), yhat.flatten())\n",
        "    re.update_state(batch_y.flatten(), yhat.flatten())\n",
        "\n",
        "precision = pre.result().numpy()\n",
        "recall = re.result().numpy()\n",
        "f1score = (2 * precision * recall) / (precision + recall)\n",
        "print(f\"Precision: {precision}\\nRecall: {recall}\\nF1-score: {f1score}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91bee3f4",
      "metadata": {},
      "source": [
        "# 7. CALCUL DES MÉTRIQUES AVEC SKLEARN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d07debf3",
      "metadata": {},
      "outputs": [],
      "source": [
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "for batch in test.as_numpy_iterator():\n",
        "    batch_x, batch_y = batch\n",
        "    yhat = model.predict(batch_x)\n",
        "    y_true.append(batch_y)\n",
        "    y_pred.append(yhat)\n",
        "\n",
        "# Conversion en tableaux numpy\n",
        "y_true = np.vstack(y_true)\n",
        "y_pred = np.vstack(y_pred)\n",
        "# Seuil de 0.5 pour binariser les prédictions\n",
        "y_pred_bin = (y_pred > 0.5).astype(int)\n",
        "\n",
        "# Calcul par label\n",
        "precision_per_label = precision_score(y_true, y_pred_bin, average=None)\n",
        "recall_per_label = recall_score(y_true, y_pred_bin, average=None)\n",
        "f1_per_label = f1_score(y_true, y_pred_bin, average=None)\n",
        "\n",
        "print(f\"Precision par label: {precision_per_label}\")\n",
        "print(f\"Recall par label: {recall_per_label}\")\n",
        "print(f\"F1-score par label: {f1_per_label}\")\n",
        "\n",
        "# F1-score global\n",
        "f1_macro = f1_score(y_true, y_pred_bin, average=\"macro\")\n",
        "f1_micro = f1_score(y_true, y_pred_bin, average=\"micro\")\n",
        "print(f\"F1-score macro (moyenne): {f1_macro}\")\n",
        "print(f\"F1-score micro (global): {f1_micro}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95cc77bf",
      "metadata": {},
      "source": [
        "# 8. SAUVEGARDE ET CHARGEMENT DU MODÈLE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c32c8afa",
      "metadata": {},
      "outputs": [],
      "source": [
        "model.save('Toxicity.h5')\n",
        "model = tf.keras.models.load_model('Toxicity.h5')\n",
        "print(\"Modèle chargé :\", model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e96bf84",
      "metadata": {},
      "source": [
        "# 9. FONCTION DE SCORING D'UN COMMENTAIRE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aef90db2",
      "metadata": {},
      "outputs": [],
      "source": [
        "def score_comment(comment):\n",
        "    vectorized_comment = vectorizer([comment])\n",
        "    results = model.predict(vectorized_comment)\n",
        "    text = \"\"\n",
        "    \n",
        "    for idx, col in enumerate(df.columns[2:]):\n",
        "        text += f'{col}: {results[0][idx] > 0.5}\\n'\n",
        "    return text\n",
        "\n",
        "# Exemple d'utilisation de la fonction\n",
        "print(score_comment(\"Fuck you\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5620032c",
      "metadata": {},
      "source": [
        "# 10. ÉVALUATION COMPLÉMENTAIRE AVEC ACCURACY_SCORE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "483daae9",
      "metadata": {},
      "outputs": [],
      "source": [
        "txt = \"I hate you\"\n",
        "vec_txt = vectorizer(txt)\n",
        "print(\"Prédiction pour 'I hate you' :\", model.predict(np.expand_dims(vec_txt, 0)))\n",
        "print(\"Labels :\", df.columns[2:])\n",
        "\n",
        "# Préparation des données de test pour évaluer l'accuracy\n",
        "x_test = np.expand_dims(vec_txt.numpy(), 0)\n",
        "y_test = [[1, 0, 0, 0, 0, 0]]\n",
        "for batch in test.as_numpy_iterator():\n",
        "    batch_x, batch_y = batch\n",
        "    x_test = np.concatenate((x_test, batch_x))\n",
        "    y_test = np.concatenate((y_test, batch_y))\n",
        "yhat = model.predict(x_test)\n",
        "yhat = (yhat > 0.5).astype(int)\n",
        "print(\"Accuracy :\", accuracy_score(np.array(y_test).flatten(), yhat.flatten()))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "tf-metal",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
